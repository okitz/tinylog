これは、Raft分散合意アルゴリズムとそのGoによる完全な実装について説明する連載記事のPart 2です。シリーズの投稿一覧は以下の通りです：

- Part 0: はじめに
- Part 1: 選挙
- Part 2: コマンドとログ複製（本記事）
- Part 3: 永続化と最適化
- Part 4: キー・バリューデータベース
- Part 5: 完全一度きりの配信

このパートでは、Raft実装を大幅に拡張し、クライアントから送信されたコマンドを実際に処理し、それらをRaftクラスタ全体に複製できるようにします。コード構成はPart 1と同じままです。新しい構造体や関数がいくつか追加され、既存のコードにも変更がありますが、それらはすぐに紹介し説明します。

このパートのすべてのコードはこのディレクトリにあります。

---

# クライアントとのやり取り  
クライアントとのやり取りについてはPart 0ですでに簡単に説明しました。ぜひそのセクションをもう一度読み返すことを強くお勧めします。ここでは、クライアントがリーダーをどのように見つけるかには焦点を当てず、リーダーを見つけた後に何が起こるかについて説明します。

まず用語についてですが、前述の通り、クライアントはRaftを使って一連のコマンドを複製します。これらは汎用的な状態機械への入力と見なすことができます。Raftの実装にとって、これらのコマンドは完全に任意のものであり、Goのany型で表現します。コマンドはRaftコンセンサスの過程で次のような処理を経ます：

1. まず、クライアントがコマンドをリーダーに送信します。Raftピアのクラスタでは、コマンドは通常1つのピアにのみ送信されます。
2. リーダーはそのコマンドをフォロワーに複製します。
3. 最後に、リーダーがコマンドが十分に複製された（つまり、クラスタの過半数のピアが自分のログにそのコマンドを持っていることを確認した）と判断したら、そのコマンドはコミットされ、すべてのクライアントに新しいコミットが通知されます。

コマンドの送信とコミットの非対称性に注意してください。これは今後議論する実装上の決定を検討する際に重要なポイントです。コマンドは1つのRaftピアに送信されますが、複数のピア（具体的には、すべての接続されている／稼働中のピア）がしばらくしてからそれをコミットし、自分のクライアントに通知します。

---

この図をPart 0から思い出してください：

===Codeblock-01===

状態機械は、Raftによる複製を利用する任意のサービスを表します。例えば、これはキー・バリューデータベースである場合もあります。コミットされたコマンドはサービスの状態を変更します（例：データベースにキー／バリューのペアを追加するなど）。

Raft ConsensusModuleの文脈でクライアントという場合、通常はこのサービスを指します。なぜなら、コミットの通知がここに報告されるからです。言い換えれば、Consensus Moduleからサービス状態機械への黒い矢印がその通知です。

もう一つのクライアントの概念として、サービスのクライアント（例：キー・バリューデータベースの利用者）があります。サービスがそのクライアントとどのようにやり取りするかはサービス自身の問題です。本記事ではRaftとサービス間のやり取りのみに焦点を当てます。

---
# 実装：コミットチャネル  
実装では、ConsensusModuleが作成される際にコミットチャネルを受け取ります。これは、コミットされたコマンドを呼び出し元に送信するためのチャネルです：commitChan chan<- CommitEntry。

CommitEntryは次のように定義されています：

```golang
// CommitEntry is the data reported by Raft to the commit channel. Each commit
// entry notifies the client that consensus was reached on a command and it can
// be applied to the client's state machine.
type CommitEntry struct {
  // Command is the client command being committed.
  Command any

  // Index is the log index at which the client command is committed.
  Index int

  // Term is the Raft term at which the client command is committed.
  Term int
}
```

このようなチャネルを使うのは設計上の選択ですが、唯一の方法ではありません。コールバックを使うこともできます。ConsensusModuleを作成する際に、呼び出し元がコールバック関数を登録し、コミットすべきコマンドがあるたびにそれを呼び出すことも可能です。

チャネルにエントリを送信するコードは後ほど紹介します。まずは、Raftサーバがどのようにコマンドを複製し、コミットされたと判断するかを説明します。

---

# Raftログ  
Raftログについてはこれまで何度か言及してきましたが、詳細には触れていませんでした。ログとは、状態機械に適用されるべきコマンドの線形なシーケンスです。必要に応じて、ログがあれば状態機械をある初期状態から「リプレイ」することができます。通常の運用中、すべてのRaftピアのログは同一です。リーダーが新しいコマンドを受け取ると、それを自身のログに追加し、フォロワーに複製します。フォロワーはコマンドを自分のログに追加し、リーダーに確認応答を返します。リーダーは、クラスタ内の過半数のサーバに安全に複製された最新のログインデックスを管理します。

Raft論文には、以下のようなログの図がいくつか掲載されています：



各ボックスはログエントリを表し、ボックスの上部の数字はそのエントリがログに追加された任期（Part 1で説明したterm）です。下部はこのログが含むキー・バリューコマンドです。各ログエントリには線形なインデックスがあります[2]。ボックスの色はtermの別表現です。

このログを空のキー・バリューストアに適用すると、最終的にx=4, y=7という値になります。

実装では、ログエントリは次のように表現されます：

```go
type LogEntry struct {
  Command any
  Term    int
}
```

そして各ConsensusModuleのログは単にlog []LogEntryです。クライアントは通常termを気にしませんが、Raftの正しさには重要なので、コードを読む際には意識しておく必要があります。

---

# 新しいコマンドの送信  
新しいコマンドをクライアントが送信できるようにする新しいSubmitメソッドから始めましょう：

```go
func (cm *ConsensusModule) Submit(command any) bool {
  cm.mu.Lock()
  defer cm.mu.Unlock()

  cm.dlog("Submit received by %v: %v", cm.state, command)
  if cm.state == Leader {
    cm.log = append(cm.log, LogEntry{Command: command, Term: cm.currentTerm})
    cm.dlog("... log=%v", cm.log)
    return true
  }
  return false
}
```

非常にシンプルです。このCMがリーダーであれば、新しいコマンドをログに追加し、trueを返します。それ以外の場合は無視され、falseが返されます。

Q: Submitからtrueが返された場合、クライアントはコマンドがリーダーに送信されたと十分に判断できますか？

A: 残念ながらそうではありません。まれに、リーダーが他のRaftサーバから分断され、新しいリーダーが選出されることがあります。しかしクライアントは依然として古いリーダーにアクセスしているかもしれません。クライアントは、送信したコマンドがコミットチャネルに現れるのを適切な時間待つべきです。現れなければ、誤ったリーダーにアクセスしたことを意味し、別のリーダーで再試行すべきです。

---

# ログエントリの複製  
リーダーに送信された新しいコマンドはログの末尾に追加されますが、この新しいコマンドはどのようにフォロワーに伝わるのでしょうか？リーダーが従う手順は、Raft論文の「Rules for Servers」セクションのFigure 2に正確に記載されています。実装では、これをleaderSendHeartbeatsで行っています。これが新しいメソッドです[3]：

===Codeblock-06===

これはPart 1で行ったことよりも複雑ですが、論文のFigure 2に従っているだけです。いくつかポイントを挙げます：

- AE RPCの各フィールドはすべて埋められています。意味は論文のFigure 2を参照してください。
- AEの応答にはsuccessフィールドがあり、フォロワーがprevLogIndexとprevLogTermに一致するエントリを見つけたかどうかをリーダーに伝えます。これに基づき、リーダーはこのフォロワーのnextIndexを更新します。
- commitIndexは、特定のログインデックスを複製したフォロワーの数に基づいて更新されます。過半数が複製していれば、commitIndexが進みます。

この部分は、前述のクライアントとのやり取りの議論と特に関係があります：

```go
if cm.commitIndex != savedCommitIndex {
  cm.dlog("leader sets commitIndex := %d", cm.commitIndex)
  cm.newCommitReadyChan <- struct{}{}
}
```

newCommitReadyChanは、CMがクライアントへのコミットチャネルに新しいエントリを送信する準備ができたことを内部的に通知するためのチャネルです。これは、CM起動時にゴルーチンで実行される次のメソッドで処理されます：

```go
func (cm *ConsensusModule) commitChanSender() {
  for range cm.newCommitReadyChan {
    // Find which entries we have to apply.
    cm.mu.Lock()
    savedTerm := cm.currentTerm
    savedLastApplied := cm.lastApplied
    var entries []LogEntry
    if cm.commitIndex > cm.lastApplied {
      entries = cm.log[cm.lastApplied+1 : cm.commitIndex+1]
      cm.lastApplied = cm.commitIndex
    }
    cm.mu.Unlock()
    cm.dlog("commitChanSender entries=%v, savedLastApplied=%d", entries, savedLastApplied)

    for i, entry := range entries {
      cm.commitChan <- CommitEntry{
        Command: entry.Command,
        Index:   savedLastApplied + i + 1,
        Term:    savedTerm,
      }
    }
  }
  cm.dlog("commitChanSender done")
}
```

このメソッドはlastApplied状態変数を更新し、すでにクライアントに送信したエントリを記録し、新しいものだけを送信します。

---

# フォロワー側のログ更新  
リーダーによる新しいログエントリの処理を見てきました。次はフォロワー側、特にAppendEntries RPCのコードを見てみましょう。以下のコード例では、Part 1から新たに追加された行が含まれています：

===Codeblock-09===

このコードは論文のFigure 2（AppendEntriesの受信側実装セクション）のアルゴリズムに忠実で、コメントも充実しています。

リーダーのLeaderCommitが自身のcm.commitIndexより大きい場合にch.newCommitReadyChanへ送信している点に注目してください。これは、フォロワーがリーダーが追加のエントリをコミット済みと判断したことを知ったタイミングです。

リーダーがAEで新しいログエントリを送信すると、次のことが起こります：

1. フォロワーは新しいエントリを自分のログに追加し、リーダーにsuccess=trueで応答します。
2. その結果、リーダーはこのフォロワーのmatchIndexを更新します。十分な数のフォロワーが次のインデックスでmatchIndexを持つと、リーダーはcommitIndexを更新し、次のAE（leaderCommitフィールド）で全フォロワーに送信します。
3. フォロワーがこれまで見たことのない新しいleaderCommitを受け取ると、新しいログエントリがコミットされたことを認識し、それらをコミットチャネルでクライアントに送信します。

Q: 新しいコマンドをコミットするのに必要なRPCのラウンドトリップ数はいくつですか？

A: 2回です。最初のラウンドでリーダーが次のログエントリをフォロワーに送り、フォロワーがそれを確認します。リーダーがAEの応答を処理すると、応答に基づいてcommitIndexを更新する場合があります。2回目のラウンドで更新されたcommitIndexがフォロワーに送信され、フォロワーはこれらのエントリをコミット済みとし、コミットチャネルで送信します。上記のコードスニペットに戻り、これらのステップを実装している箇所を探してみてください。

---

# 選挙の安全性  
ここまででログ複製をサポートするために追加された新しいコードを見てきましたが、ログはRaftの選挙にも影響します。Raft論文の5.4.1節「Election restriction」では、Raftは候補者のログがクラスタの過半数のピアと同等以上に最新でなければ選挙に勝てないようにしています[4]。

このため、RVにはlastLogIndexとlastLogTermフィールドがあります。候補者がRVを送信する際、これらに自身の最後のログエントリ情報を入れます。フォロワーはこれらのフィールドを自分のものと比較し、候補者が十分に最新かどうかを判断します。

以下は新しいstartElectionで、追加された行が含まれています：

===Codeblock-10===

そしてlastLogIndexAndTermは新しいヘルパーメソッドです：

===Codeblock-11===

なお、実装ではインデックスは0始まりですが、Raft論文では1始まりです。そのため-1が番兵値として使われることが多いです。

選挙の安全性チェックを実装したRVハンドラの更新版は以下の通りです：

===Codeblock-12===

---

# 「暴走サーバ」シナリオの再考  
Part 1では、3台クラスタのサーバBが数秒間切断され、150-300msごとに新たな選挙を繰り返すシナリオを説明しました。再接続時、Bのtermは他のピアよりも高くなります。

このとき、もし接続されていたピアがその間にログエントリを複製していた場合を考えてみましょう。

Bがクラスタに戻ると再選挙が発生しますが（リーダーはAE応答でより高いtermを見てフォロワーに戻る）、BのログはAやCよりも不完全なので選挙に勝てません。これは前述の選挙安全性チェックによるものです。AかCが再選挙に勝つため、クラスタの混乱は最小限に抑えられます。

この不要な混乱が気になる場合、Ongaroの論文では「サーバがクラスタに再参加した際の混乱防止」というセクションでこの問題を議論しています。一般的な解決策は「プリ・ボート」ステップで、サーバが候補者になる前にいくつかのチェックを行います。

これはまれなケースへの最適化なので、ここでは深く触れません。興味があれば論文を参照してください。

---

# Q&A  
最後に、Raftの学習や実装時によくある質問をいくつか紹介します。追加の質問があればメールで送ってください。よくあるものは記事に追記します。

Q: commitIndexとlastAppliedはなぜ分かれているのですか？RPC（やその応答）でcommitIndexが進んだ分だけクライアントにコマンドを送れば十分では？

A: この2つを分けることで、迅速な処理（RPC処理）と遅くなりうる処理（クライアントへのコマンド送信）を切り離しています。フォロワーがAEを受けてリーダーのcommitIndexが自分より大きいと知ったとき、複数のログコマンドをコミットチャネルに送ることができます。しかしチャネルへの送信（やコールバック呼び出し）はブロッキングの可能性があるため、RPCにはすぐ応答したいのです。lastAppliedでこれを分離します。RPCはcommitIndexだけを更新し、commitChanSenderゴルーチンがバックグラウンドで新しいコミット済みコマンドをクライアントに送ります。

同じことがnewCommitReadyChanにも当てはまるかもしれませんが、このチャネルはバッファ付きで、両端を制御できるため小さなバッファで高確率でブロックしません。ただし、極端に遅いクライアントの場合はRPCが遅れることもありますが、これは自然なバックプレッシャーとなります。

Q: リーダーでnextIndexとmatchIndexは両方必要ですか？

A: matchIndexだけでもアルゴリズムは動きますが、非効率です。リーダー交代時、新リーダーはピアの最新状態を仮定できず、matchIndexを-1で初期化します。全ログを各フォロワーに送ろうとしますが、ほとんどのフォロワーはほぼ同じログを持っているはずです。nextIndexがあれば、ログの末尾から効率的に差分だけを送ることができます。

---

# 今後について  
ぜひコードを動かしてテストし、ログを観察してみてください。

現時点でRaft実装はほぼ完成ですが、永続化には未対応です。つまり、サーバがクラッシュして再起動した場合に脆弱です。

Raftアルゴリズムはこれに対応しており、Part 3で解説します。永続化を追加することで、最悪のタイミングでサーバがクラッシュするようなテストも可能になります。

また、Part 3では本記事の実装に対する最適化も扱います。特に、リーダーは新しい情報があるときはより迅速にAEを送るべきですが、現状は50msごとにしか送っていません。これも修正します。

---